{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "172f9ff0-71bd-4a75-8072-fafedfdff0c9",
   "metadata": {},
   "source": [
    "## 文書分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8da8a26-609e-429f-8c93-75d92d914704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakaimasayuki/projects/private/nlp/introduction-to-large-language-models/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1796a907-bca1-43b0-a9e3-c81e5bf79c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classification_pipeline = pipeline(\n",
    "    model=\"llm-book/bert-base-japanese-v3-marc_ja\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952610d7-fd6f-4bfd-9efd-b4cae526e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'positive', 'score': 0.9991204142570496}\n"
     ]
    }
   ],
   "source": [
    "positive_text = \"明日の運動会が楽しみだ。\"\n",
    "print(text_classification_pipeline(positive_text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216e4f57-c2fa-40b2-8cda-d4f18b895902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'negative', 'score': 0.9057907462120056}\n"
     ]
    }
   ],
   "source": [
    "negative_text = \"明日の遠足は雨予報なので行きたくない。\"\n",
    "print(text_classification_pipeline(negative_text)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語推論\n",
    "\n",
    "二つのテキストの論理関係を予測するタスク。\n",
    "\n",
    "* `llm-book/bert-base-japanese-v3-jnli`を使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_pipeline = pipeline(\n",
    "    model=\"llm-book/bert-base-japanese-v3-jnli\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'entailment', 'score': 0.9964311122894287}\n"
     ]
    }
   ],
   "source": [
    "text = \"二人の男性がジェット機を見ています\"\n",
    "entailment_text = \"ジェット機を見ている人が二人います\"\n",
    "# entailment: 含意\n",
    "print(nli_pipeline({\n",
    "    \"text\": text,\n",
    "    \"text_pair\": entailment_text\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'contradiction', 'score': 0.9990535378456116}\n"
     ]
    }
   ],
   "source": [
    "contradition_text = \"二人の男性が飛んでいます\"\n",
    "# contradition: 矛盾\n",
    "print(nli_pipeline({\n",
    "    \"text\": text,\n",
    "    \"text_pair\": contradition_text\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'neutral', 'score': 0.9959145188331604}\n"
     ]
    }
   ],
   "source": [
    "neutral_text = \"2人の男性が、白い飛行機を眺めています\"\n",
    "# neutral: 中立\n",
    "print(nli_pipeline(\n",
    "    {\"text\": text,\n",
    "    \"text_pair\": neutral_text}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 意味的類似度計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sim_pipeline = pipeline(\n",
    "    model=\"llm-book/bert-base-japanese-v3-jsts\",\n",
    "    function_to_apply=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'LABEL_0', 'score': 3.258415937423706}\n"
     ]
    }
   ],
   "source": [
    "text = \"川べりでサーフボードを持った人たちがいます\"\n",
    "sim_text = \"サーファーたちが川べりにたっています\"\n",
    "\n",
    "result = text_sim_pipeline({\n",
    "    \"text\": text,\n",
    "    \"text_pair\": sim_text\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'LABEL_0', 'score': 0.04162188991904259}\n"
     ]
    }
   ],
   "source": [
    "dissim_text = \"トイレの壁に黒いタオルがかけられています\"\n",
    "\n",
    "result = text_sim_pipeline({\n",
    "    \"text\": text,\n",
    "    \"text_pair\": dissim_text\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 634/634 [00:00<00:00, 566kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 445M/445M [00:12<00:00, 34.3MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 529/529 [00:00<00:00, 1.46MB/s]\n",
      "vocab.txt: 100%|██████████| 231k/231k [00:00<00:00, 38.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 527kB/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "sim_enc_pipeline = pipeline(\n",
    "    model=\"llm-book/bert-base-japanese-v3-unsup-simcse-jawiki\",\n",
    "    task=\"feature-extraction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8597443699836731\n"
     ]
    }
   ],
   "source": [
    "text_emb = sim_enc_pipeline(text, return_tensors=True)[0][0]\n",
    "sim_emb = sim_enc_pipeline(sim_text, return_tensors=True)[0][0]\n",
    "\n",
    "# calculate cosine similarity\n",
    "sim_pair_score = cosine_similarity(text_emb, sim_emb, dim=0)\n",
    "print(sim_pair_score.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4588705599308014\n"
     ]
    }
   ],
   "source": [
    "dissim_emb = sim_enc_pipeline(dissim_text, return_tensors=True)[0][0]\n",
    "dissim_pair_score = cosine_similarity(text_emb, dissim_emb, dim=0)\n",
    "print(dissim_pair_score.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固有表現認識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.93k/1.93k [00:00<00:00, 3.23MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 443M/443M [00:12<00:00, 36.9MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 529/529 [00:00<00:00, 1.86MB/s]\n",
      "vocab.txt: 100%|██████████| 231k/231k [00:00<00:00, 707kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 462kB/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TokenClassificationPipeline._sanitize_parameters() got an unexpected keyword argument 'aggregation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sakaimasayuki/projects/private/nlp/introduction-to-large-language-models/chapter-01/.ipynb_checkpoints/document-classification-checkpoint.ipynb セル 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sakaimasayuki/projects/private/nlp/introduction-to-large-language-models/chapter-01/.ipynb_checkpoints/document-classification-checkpoint.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpprint\u001b[39;00m \u001b[39mimport\u001b[39;00m pprint\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sakaimasayuki/projects/private/nlp/introduction-to-large-language-models/chapter-01/.ipynb_checkpoints/document-classification-checkpoint.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ner_pipeline \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sakaimasayuki/projects/private/nlp/introduction-to-large-language-models/chapter-01/.ipynb_checkpoints/document-classification-checkpoint.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllm-book/bert-base-japanese-v3-ner-wikipedia-dataset\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sakaimasayuki/projects/private/nlp/introduction-to-large-language-models/chapter-01/.ipynb_checkpoints/document-classification-checkpoint.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     aggregation\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msimple\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sakaimasayuki/projects/private/nlp/introduction-to-large-language-models/chapter-01/.ipynb_checkpoints/document-classification-checkpoint.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[0;32m~/projects/private/nlp/introduction-to-large-language-models/.venv/lib/python3.11/site-packages/transformers/pipelines/__init__.py:1070\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1068\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m device\n\u001b[0;32m-> 1070\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline_class(model\u001b[39m=\u001b[39;49mmodel, framework\u001b[39m=\u001b[39;49mframework, task\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/private/nlp/introduction-to-large-language-models/.venv/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:136\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__init__\u001b[0;34m(self, args_parser, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, args_parser\u001b[39m=\u001b[39mTokenClassificationArgumentHandler(), \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 136\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_model_type(\n\u001b[1;32m    138\u001b[0m         TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES\n\u001b[1;32m    139\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[39melse\u001b[39;00m MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_basic_tokenizer \u001b[39m=\u001b[39m BasicTokenizer(do_lower_case\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/private/nlp/introduction-to-large-language-models/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:830\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_size \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    829\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_workers \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 830\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_params, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_params, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_postprocess_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_parameters(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    832\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_processor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractor, BaseImageProcessor):\n\u001b[1;32m    834\u001b[0m         \u001b[39m# Backward compatible change, if users called\u001b[39;00m\n\u001b[1;32m    835\u001b[0m         \u001b[39m# ImageSegmentationPipeline(.., feature_extractor=MyFeatureExtractor())\u001b[39;00m\n\u001b[1;32m    836\u001b[0m         \u001b[39m# then we should keep working\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: TokenClassificationPipeline._sanitize_parameters() got an unexpected keyword argument 'aggregation'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    model=\"llm-book/bert-base-japanese-v3-ner-wikipedia-dataset\",\n",
    "    aggregation=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"大谷翔平は岩手県水沢市出身のプロ野球選手\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
